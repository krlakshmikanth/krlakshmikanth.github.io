---
layout: post
title: Dealing with bias in AI algorithms
tag:
- inclusive-ai
- ai-decision-making
- algorithmic-bias
- gender-bias-in-tech
---

<p>This blog is inspired by a ted video presented by <a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a></p><h4>Introduction about <a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a>,</h4><p>She’s an MIT grad student working with facial analysis software when she noticed a problem: the software didn’t detect her face — because the people who coded the algorithm hadn’t taught it to identify a broad range of skin tones and facial structures. Now she’s on a mission to fight bias in machine learning, a phenomenon she calls the “coded gaze.” It’s an eye-opening talk about the need for accountability in coding … as algorithms take over more and more aspects of our lives. Her research explores the intersection of social impact technology and inclusion.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ksI7dNZsINxXYgY3BLGZfA.jpeg" /><figcaption><a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a></figcaption></figure><h4>What’s Algorithmic bias?</h4><p>Algorithmic bias describes <strong>systematic</strong> and <strong>repeatable errors</strong> in a computer system that create <strong>unfair outcomes</strong>, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the<strong> design of the algorithm</strong> or the unintended or unanticipated use or decisions relating to the way data is <strong>coded, collected, selected</strong>, or used to <strong>train the algorithm</strong>. Algorithmic bias is found across platforms, including but not limited to <a href="https://en.wikipedia.org/wiki/Search_engine_bias">search engine results</a> and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing <a href="https://en.wikipedia.org/wiki/Bias">social biases</a> of <strong>race, gender, sexuality, and ethnicit</strong>y. The study of algorithmic bias is most concerned with algorithms that reflect “systematic and unfair” discrimination.</p><p><strong>Why Algorithmic Bias matter?<br></strong>It started when <a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> wanted to test her face on a camera-enabled to detect/identify the face of humans for computational processing; wherein the camera detected the people with light skin and demanded dark skin people to wear a white mask to detect their faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*41ZoldQNgLyrhJGHaJgjRQ.png" /><figcaption><a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> involved in a project during her undergraduate to enable a robot to play peek-a-boo</figcaption></figure><p>It happened to her once she was involved in a project during her undergraduate program to train a robot to play peek-a-boo (where robot played actively with her roommates of lighter skin and failed to detect her face often)and it again happened to her at an entrepreneurship competition.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g2ZpqBPUlbA9NEinMjWMEA.png" /><figcaption><a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> at an entrepreneur competition (Hong Kong)</figcaption></figure><h4>What’s the Aspire Mirror?</h4><p><strong>Aspire mirror</strong> (a project by <a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a>) is a device that enables you to look at yourself and see a reflection on your face based on what inspires you or what you hope to empathize with. Where she used a generic facial recognition software and it couldn’t detect her in a row; the solution for this problem would be creating a full-spectrum training set that will reflect a richer portrait of humanity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/888/1*Q77ljcftlybstqc-j7Bh-g.jpeg" /><figcaption>Aspire Mirror Project Picture as available in the internet</figcaption></figure><h4>Why people should read this book “Weapons of Math destruction” written by Cathy O’Neil?</h4><p>Cause the author talks about the importance of WMD (Widespread, Mysterious, and Destructive Algorithms). She analyses how the use of <a href="https://en.wikipedia.org/wiki/Big_data">big data</a> and <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> in a variety of fields, including insurance, advertising, education, and policing, can lead to decisions that harm the poor, reinforce <a href="https://en.wikipedia.org/wiki/Racism">racism</a>, and amplify inequality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/1*LrRNzzHXOnrIpOBCnHghFA.jpeg" /><figcaption>Weapons of Math Destruction by Cathy O’Neil</figcaption></figure><p>Most troubling, they reinforce discrimination: If a poor student can’t get a loan because a lending model deems him too risky (by his zip code), he’s then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a “toxic cocktail for democracy.”</p><p><a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> wants people to be an inclusive coder. She focuses on three aspects of developing things concerning inclusion and they’re</p><ol><li>Who code matters</li><li>How we code matters</li><li>Why we code matters</li></ol><p>By following the above aspect we have an option to unlock the inequality if we consider a social change a priority. Also, we should try identifying the bias and curate inclusiveness with conscientious development to fight against the inequality in the algorithm inferences. <a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> wants people to join Algorithmic Justice League (<a href="https://ajlunited.org">https://ajlunited.org</a>) to fight against the inequality in the algorithms available in the market by conveying that <strong>technology should serve all of us and not just the privileged few.</strong></p><h4>What is Gender Shades?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/483/1*v-tV6tBJAx-VmIoeQPceMA.png" /><figcaption><a href="https://medium.com/u/6343c11d8f21">Joy Buolamwini</a> and <a href="https://medium.com/u/2e2b7ed6c4f5">Timnit Gebru</a>; researchers from Gender Shades project</figcaption></figure><p>Gender Shades in detail: <strong>Intersectional Accuracy Disparities in Commercial Gender Classification. </strong>Gender Shades is an approach to evaluate bias present in automated facial analysis algorithms and datasets concerning phenotypic subgroups.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*iJMCbe4j3ga0G1PY9KwRrw.png" /></figure><p>From the abstract: The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/885/1*Q31H5i7CT52ZPCx6Oarqcw.png" /><figcaption>Research outcome from Gender Shades</figcaption></figure><p>Gender Shades research worked on the models that are made available for commercial needs from Microsoft, Face++, and IBM. And the final accuracy metrics were <strong>Microsoft (93.7%) &gt; Face++ (90.0%) &gt; IBM had (87.9%) </strong>From the results, all companies performed better on males and even better results with lighter skin than darker ones. In particular, all the companies performed badly at darker females. One of the finest practices for gender inequality in real-time was, <strong>Woman, are less likely to be shown ads for high paying jobs on Google</strong> (since most of the executives are male-dominant — an algorithm learns in such a way)</p><h4>Final thoughts,</h4><blockquote>We’re entered into a world of automation with over confident and under prepared. We should develop/ train machine make contextual decisions with inclusiveness and practicality in nature. Machine neutrality is expected across the globe and hence technology should serve all of us and not just the privileged few.</blockquote><h4>Credits</h4><ol><li>Algorithmic Bias <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">https://en.wikipedia.org/wiki/Algorithmic_bias</a></li><li>Joy Buolamwini Image <a href="https://upload.wikimedia.org/wikipedia/commons/a/ac/Joy_Buolamwini_-_Wikimania_2018_01.jpg">https://upload.wikimedia.org/wikipedia/commons/a/ac/Joy_Buolamwini_-_Wikimania_2018_01.jpg</a></li><li>Ted Talk <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en">https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en</a></li><li>Algorithmic Justice League <a href="https://ajlunited.org">https://ajlunited.org</a></li><li>Gender Shades <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf</a></li><li>When the Robot Doesn’t See Dark Skin <a href="https://www.nytimes.com/2018/06/21/opinion/facial-analysis-technology-bias.html">https://www.nytimes.com/2018/06/21/opinion/facial-analysis-technology-bias.html</a></li><li>AI, Ain’t I a Woman <a href="https://www.youtube.com/watch?v=QxuyfWoVV98">https://www.youtube.com/watch?v=QxuyfWoVV98</a></li><li>Not Flawless <a href="https://www.notflawless.ai">https://www.notflawless.ai</a></li><li>Aspire Mirror <a href="http://www.aspiremirror.com">http://www.aspiremirror.com</a></li><li>Race &amp; Gender in Tech <a href="http://www.sigcas.org/wp-content/uploads/2019/05/LeeCSG2019.pdf">http://www.sigcas.org/wp-content/uploads/2019/05/LeeCSG2019.pdf</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e019e2d202ea" width="1" height="1" alt="">
